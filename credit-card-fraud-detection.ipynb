

Kaggle
Credit Card Fraud Detection: A Hands-On Project

Credit card fraud is a major concern for banks and financial institutions. Fraudsters use various techniques to steal credit card information and make unauthorized transactions. In this project, we will explore a dataset containing credit card transactions and build models to predict fraudulent transactions.

We will use the Kaggle dataset Credit Card Fraud Detection which contains credit card transactions made by European cardholders. The dataset consists of 284,807 transactions, out of which 492 are fraudulent. The data contains only numerical input variables which are a result of Principal Component Analysis (PCA) transformations due to confidentiality issues. The features include 'Time', 'Amount', and 'V1' through 'V28', as well as the 'Class' variable, which is the target variable indicating whether the transaction is fraudulent (1) or not (0).

In this project, we will start with exploratory data analysis (EDA) to get a better understanding of the data. Next, we will perform data processing and modeling, where we will build several classification models to predict fraudulent transactions. We will also address the issue of imbalanced classes by using undersampling. Finally, we will evaluate the performance of the models and choose the best one based on various evaluation metrics such as precision, recall, F1-score, and accuracy.
READ MORE

# Import necessary libraries
%matplotlib inline
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
import sklearn.metrics as metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, precision_recall_curve, f1_score, fbeta_score, accuracy_score

# Set plot style
plt.style.use('ggplot')

# Turn off warnings
import warnings
warnings.filterwarnings('ignore')

# Set font size for all plots
plt.rcParams['font.size'] = 12
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10

/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

# Loading data
df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')

1. Exploratory Data Analysis

# Printing random sample of 10 rows to check data loading
df.sample(10)

	Time 	V1 	V2 	V3 	V4 	V5 	V6 	V7 	V8 	V9 	... 	V21 	V22 	V23 	V24 	V25 	V26 	V27 	V28 	Amount 	Class
224492 	143822.0 	1.827193 	-0.796064 	-0.847914 	-0.071583 	-0.768242 	-1.040428 	-0.147448 	-0.255327 	0.986605 	... 	0.029048 	-0.033629 	0.158055 	0.123884 	-0.471259 	1.143338 	-0.126022 	-0.044591 	126.32 	0
84462 	60318.0 	1.331055 	-0.525019 	0.052520 	-0.492047 	-0.812402 	-1.054523 	-0.147736 	-0.357991 	-0.896496 	... 	0.189989 	0.534911 	-0.218684 	0.471137 	0.801299 	-0.073386 	-0.012896 	0.014415 	60.00 	0
54849 	46655.0 	1.386852 	-0.910458 	0.380348 	-1.469440 	-1.169872 	-0.490805 	-0.750913 	-0.085650 	-2.475861 	... 	-0.530566 	-1.234978 	0.304678 	0.154473 	-0.022269 	-0.646853 	0.032326 	0.018657 	34.90 	0
95972 	65546.0 	1.215988 	-0.090210 	0.339281 	-0.230334 	-0.418672 	-0.370819 	-0.240249 	0.079758 	0.067246 	... 	-0.037898 	-0.114629 	0.070630 	0.076375 	0.121154 	0.974453 	-0.074269 	-0.011320 	0.92 	0
190179 	128727.0 	-1.188129 	1.919761 	-1.696294 	-0.123108 	0.616486 	-1.812508 	0.387103 	0.529756 	-0.325118 	... 	0.228845 	0.570801 	-0.227048 	-0.230647 	0.223449 	-0.139821 	-0.161042 	0.022871 	1.00 	0
49256 	43979.0 	-0.285047 	1.082357 	1.448482 	-0.032454 	0.450999 	-0.457090 	0.904839 	-0.212668 	-0.408351 	... 	-0.276766 	-0.463429 	-0.039894 	0.051443 	-0.225539 	0.069686 	0.105829 	-0.126085 	2.58 	0
76293 	56499.0 	1.221303 	0.073936 	0.974213 	1.129239 	-0.434135 	0.307537 	-0.477134 	0.055712 	0.713983 	... 	-0.102174 	0.003190 	-0.123330 	-0.415348 	0.580808 	-0.361373 	0.082474 	0.032290 	4.99 	0
239834 	150291.0 	2.052440 	-1.197022 	-0.437060 	-0.826165 	-1.335505 	-0.531289 	-1.147997 	0.072494 	0.062988 	... 	0.387280 	1.055715 	0.132667 	0.021835 	-0.260548 	-0.105919 	-0.003487 	-0.055848 	39.00 	0
259010 	158917.0 	-0.749603 	0.766213 	1.021219 	-0.415385 	0.742919 	0.074781 	0.721020 	0.099401 	0.183808 	... 	0.147364 	0.693906 	-0.652988 	0.623166 	1.067874 	0.952008 	-0.007576 	0.066815 	20.20 	0
52445 	45456.0 	-3.860470 	2.443696 	-0.375988 	-2.238683 	-1.043765 	1.072767 	-2.590528 	-8.653390 	-0.200332 	... 	8.498610 	-2.109474 	0.819644 	-0.119207 	0.229054 	-0.227300 	1.052361 	0.516669 	85.75 	0

10 rows × 31 columns

✅ We can only work with three non-transformed variables which are Time, Amount, and Class (where Class takes values of 1 for fraud and 0 for not fraud).

# Printing data overview
df.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
 #   Column  Non-Null Count   Dtype  
---  ------  --------------   -----  
 0   Time    284807 non-null  float64
 1   V1      284807 non-null  float64
 2   V2      284807 non-null  float64
 3   V3      284807 non-null  float64
 4   V4      284807 non-null  float64
 5   V5      284807 non-null  float64
 6   V6      284807 non-null  float64
 7   V7      284807 non-null  float64
 8   V8      284807 non-null  float64
 9   V9      284807 non-null  float64
 10  V10     284807 non-null  float64
 11  V11     284807 non-null  float64
 12  V12     284807 non-null  float64
 13  V13     284807 non-null  float64
 14  V14     284807 non-null  float64
 15  V15     284807 non-null  float64
 16  V16     284807 non-null  float64
 17  V17     284807 non-null  float64
 18  V18     284807 non-null  float64
 19  V19     284807 non-null  float64
 20  V20     284807 non-null  float64
 21  V21     284807 non-null  float64
 22  V22     284807 non-null  float64
 23  V23     284807 non-null  float64
 24  V24     284807 non-null  float64
 25  V25     284807 non-null  float64
 26  V26     284807 non-null  float64
 27  V27     284807 non-null  float64
 28  V28     284807 non-null  float64
 29  Amount  284807 non-null  float64
 30  Class   284807 non-null  int64  
dtypes: float64(30), int64(1)
memory usage: 67.4 MB

# Printing numerical summary for Time and Amount columns
df.loc[:, ['Time', 'Amount']].describe()

	Time 	Amount
count 	284807.000000 	284807.000000
mean 	94813.859575 	88.349619
std 	47488.145955 	250.120109
min 	0.000000 	0.000000
25% 	54201.500000 	5.600000
50% 	84692.000000 	22.000000
75% 	139320.500000 	77.165000
max 	172792.000000 	25691.160000

✅ From the plot, we can observe that the Time feature has a bimodal distribution with two peaks, indicating that there are two periods during the day when credit card transactions are more frequent. The first peak occurs at around 50,000 seconds (approximately 14 hours), while the second peak occurs at around 120,000 seconds (approximately 33 hours). This suggests that there may be a pattern in the timing of credit card transactions that could be useful for fraud detection.

# Plotting distribution of Time feature
plt.figure(figsize=(10,8), )
plt.title('Time Distribution (Seconds)')
sns.distplot(df['Time'], color='blue')

# Save the plot as PNG file
plt.savefig('time_distribution.png');

✅ From the plot, we can observe that the distribution of the Amount feature is highly skewed to the right, with a long tail to the right. This indicates that the majority of the transactions have low amounts, while a few transactions have extremely high amounts. As a result, this suggests that the dataset contains some outliers in terms of transaction amounts. Therefore, when building a model for fraud detection, it may be necessary to handle outliers in the Amount feature, for instance, by using a log transformation or robust statistical methods.

# Plotting distribution of Amount feature
plt.figure(figsize=(10,8))
plt.title('Distribution of Amount')
sns.distplot(df['Amount'], color='blue')

# Save the plot as PNG file
plt.savefig('amount_distribution.png');

# Counting number of fraud vs non-fraud transactions and displaying them with their ratio
fraud = df['Class'].value_counts()[1]
nonfraud = df['Class'].value_counts()[0]
print(f'Fraudulent: {fraud}, Non-fraudulent: {nonfraud}')
print(f'Ratio of fraud to non-fraud: {fraud}/{nonfraud} ({fraud/nonfraud*100:.3f}%)')

Fraudulent: 492, Non-fraudulent: 284315
Ratio of fraud to non-fraud: 492/284315 (0.173%)

✅ From the plot, we can observe that the dataset is highly imbalanced, with a vast majority of transactions being non-fraudulent (class 0) and a relatively small number of transactions being fraudulent (class 1). This indicates that the dataset has a class imbalance problem, which may affect the performance of a model trained on this dataset. It may be necessary to use techniques such as oversampling, undersampling, or class weighting to handle the class imbalance problem when building a model for fraud detection.

# Plotting count of fraud vs non-fraud transactions in a bar chart
plt.figure(figsize=(10,8))
sns.barplot(x=df['Class'].value_counts().index, y=df['Class'].value_counts(), color='blue')
plt.title('Fraudulent vs. Non-Fraudulent Transactions')
plt.ylabel('Count')
plt.xlabel('0:Non-Fraudulent, 1:Fraudulent')

# Save the plot as PNG file
plt.savefig('fraud_vs_nonfraud_transactions.png');

2. Data Processing

✅ From the heatmap, it can be observed that there are no strong positive or negative correlations between any pairs of variables in the dataset. The strongest correlations are found:

    Time and V3, with a correlation coefficient of -0.42
    Amount and V2, with a correlation coefficient of -0.53
    Amount and V4, with a correlation coefficient of 0.4. 

Although these correlations are relatively high, the risk of multicollinearity is not expected to be significant. Overall, the heatmap suggests that there are no highly correlated variables that need to be removed before building a machine learning model.

# Plotting heatmap to find any high correlations between variables
plt.figure(figsize=(10,8))
sns.heatmap(data=df.corr(), cmap="seismic", annot=False)
# plt.show()

# Save the plot as PNG file
plt.savefig('corr_heatmap.png');

3. Modeling

✅ The "Credit Card Fraud Detection" dataset has credit card transactions labeled as fraudulent or not. The dataset is imbalanced, so it needs a model that can accurately detect fraudulent transactions without wrongly flagging non-fraudulent transactions. 

✅ To help with classification problems, StandardScaler standardizes data by giving it a mean of 0 and a standard deviation of 1, which results in a normal distribution. This technique works well when dealing with a wide range of amounts and time. To scale the data, the training set is used to initialize the fit, and the train, validation, and test sets are then scaled before running them into the models. 

✅ The dataset was divided into 60% for training, 20% for validation, and 20% for testing. To balance the imbalanced dataset, Random Undersampling was used to match the number of fraudulent transactions. Logistic Regression and Random Forest models were used, and good results were produced. 

✅ The commonly used models for the "Credit Card Fraud Detection" dataset are Logistic Regression, Naive Bayes, Random Forest, and Dummy Classifier. 

    Logistic Regression is widely used for fraud detection because of its interpretability and ability to handle large datasets. 
    Naive Bayes is commonly used for fraud detection because it can handle datasets with a large number of features and can provide fast predictions. 
    Random Forest is commonly used for fraud detection because it can handle complex datasets and is less prone to overfitting. 
    The Dummy Classifier is a simple algorithm used as a benchmark to compare the performance of other models.

# Drop the 'Class' column to prepare data for splitting
data = df.drop(columns=['Class'])

# Get the target variable
answer = df['Class']

# Split data into training, validation and test sets, ensuring the class distribution is maintained
X_trainval, X_test, y_trainval, y_test = train_test_split(data, answer
                                                          , test_size=0.2
                                                          , stratify=df['Class']
                                                          , random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval
                                                  , test_size=0.25
                                                  , stratify=y_trainval
                                                  , random_state=42)

# Initialize the StandardScaler object and fit it to the training data
scaler = StandardScaler()
scaler.fit(X_train)

# Scale the training, validation, and test sets using the scaler
X_train_std = scaler.transform(X_train)
X_val_std = scaler.transform(X_val)
X_test_std = scaler.transform(X_test)

✅ Undersampling will be utilized to address the issue of imbalanced classes.

# Undersampling will be utilized to address the issue of imbalanced classes.

# Instantiate RandomUnderSampler
rus = RandomUnderSampler(random_state=42)

# Undersample the training set
X_train_under, y_train_under = rus.fit_resample(X_train_std, y_train)

# Undersample the validation set
X_val_under, y_val_under = rus.fit_resample(X_val_std, y_val)

3.1. Logistic Regression

# Logistic Regression
# Run CV with 5 folds (logit)
penalty = ['l2']
C = np.logspace(0, 4, 10, 100, 1000)
param_grid = dict(C=C, penalty=penalty)

logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000)
logistic_grid = GridSearchCV(logistic, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)
logistic_grid.fit(X_train_under, y_train_under)

Fitting 5 folds for each of 10 candidates, totalling 50 fits

/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=10000), n_jobs=-1,
             param_grid={'C': array([1.00000000e+00, 2.15443469e+01, 4.64158883e+02, 1.00000000e+04,
       2.15443469e+05, 4.64158883e+06, 1.00000000e+08, 2.15443469e+09,
       4.64158883e+10, 1.00000000e+12]),
                         'penalty': ['l2']},
             scoring='roc_auc', verbose=10)

In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
Support Vector Machine (SVM)

# # Support Vector Machine (SVM)
# # Run CV with 5 folds (SVM)
# C = [1]
# gammas = [0.001, 0.1]
# param_grid = dict(C=C, gamma=gammas)

# svm1 = svm.SVC(kernel='rbf', probability=True)
# svm_grid = GridSearchCV(svm1, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)
# svm_grid.fit(X_train_under, y_train_under)

3.2. Naive Bayes

# Naive Bayes
# Fit a Naive Bayes Model
gnb = GaussianNB()
gnb_best = gnb.fit(X_train_under, y_train_under)

3.3. Random Forest

# Random Forest
# Run CV with 5 folds (Random Forest)
# Create the parameter grid based on the results of random search 
param_grid = {
    'max_depth': [5, 10, 15],
    'max_features': ['sqrt'],
    'min_samples_leaf': [10, 20],
    'min_samples_split': [2, 5],
    'n_estimators': [500, 700]
}

rf = RandomForestClassifier()
rf_grid = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)
rf_grid.fit(X_train_under,y_train_under)

Fitting 5 folds for each of 24 candidates, totalling 120 fits
[CV 1/5; 1/10] START C=1.0, penalty=l2..........................................
[CV 1/5; 1/10] END ...........C=1.0, penalty=l2;, score=0.994 total time=   0.0s
[CV 5/5; 1/10] START C=1.0, penalty=l2..........................................
[CV 5/5; 1/10] END ...........C=1.0, penalty=l2;, score=0.988 total time=   0.0s
[CV 2/5; 2/10] START C=21.544346900318832, penalty=l2...........................
[CV 2/5; 2/10] END C=21.544346900318832, penalty=l2;, score=0.982 total time=   0.0s
[CV 1/5; 3/10] START C=464.15888336127773, penalty=l2...........................
[CV 1/5; 3/10] END C=464.15888336127773, penalty=l2;, score=0.997 total time=   0.1s
[CV 5/5; 3/10] START C=464.15888336127773, penalty=l2...........................
[CV 5/5; 3/10] END C=464.15888336127773, penalty=l2;, score=0.988 total time=   0.1s
[CV 4/5; 4/10] START C=9999.999999999995, penalty=l2............................
[CV 4/5; 4/10] END C=9999.999999999995, penalty=l2;, score=0.958 total time=   0.3s
[CV 3/5; 5/10] START C=215443.46900318822, penalty=l2...........................
[CV 3/5; 5/10] END C=215443.46900318822, penalty=l2;, score=0.986 total time=   0.2s
[CV 3/5; 6/10] START C=4641588.833612782, penalty=l2............................
[CV 3/5; 6/10] END C=4641588.833612782, penalty=l2;, score=0.986 total time=   0.2s
[CV 1/5; 7/10] START C=99999999.9999999, penalty=l2.............................
[CV 1/5; 7/10] END C=99999999.9999999, penalty=l2;, score=0.999 total time=   0.2s
[CV 3/5; 7/10] START C=99999999.9999999, penalty=l2.............................
[CV 3/5; 7/10] END C=99999999.9999999, penalty=l2;, score=0.985 total time=   0.4s
[CV 1/5; 8/10] START C=2154434690.031878, penalty=l2............................
[CV 1/5; 8/10] END C=2154434690.031878, penalty=l2;, score=0.999 total time=   0.2s
[CV 3/5; 8/10] START C=2154434690.031878, penalty=l2............................
[CV 3/5; 8/10] END C=2154434690.031878, penalty=l2;, score=0.986 total time=   0.2s
[CV 5/5; 8/10] START C=2154434690.031878, penalty=l2............................
[CV 5/5; 8/10] END C=2154434690.031878, penalty=l2;, score=0.986 total time=   0.3s
[CV 1/5; 9/10] START C=46415888336.12772, penalty=l2............................
[CV 1/5; 9/10] END C=46415888336.12772, penalty=l2;, score=0.999 total time=   0.2s
[CV 2/5; 9/10] START C=46415888336.12772, penalty=l2............................
[CV 2/5; 9/10] END C=46415888336.12772, penalty=l2;, score=0.976 total time=   0.2s
[CV 4/5; 9/10] START C=46415888336.12772, penalty=l2............................
[CV 4/5; 9/10] END C=46415888336.12772, penalty=l2;, score=0.946 total time=   0.7s
[CV 4/5; 10/10] START C=1000000000000.0, penalty=l2.............................
[CV 4/5; 10/10] END C=1000000000000.0, penalty=l2;, score=0.941 total time=   1.6s
[CV 4/5; 1/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 4/5; 1/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.955 total time=   1.9s
[CV 1/5; 2/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 1/5; 2/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.989 total time=   2.8s
[CV 1/5; 3/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 1/5; 3/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.989 total time=   2.0s
[CV 4/5; 3/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 4/5; 3/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.956 total time=   1.9s
[CV 2/5; 4/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 2/5; 4/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.977 total time=   2.6s
[CV 1/5; 5/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 1/5; 5/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.989 total time=   1.8s
[CV 4/5; 5/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 4/5; 5/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.949 total time=   1.8s
[CV 3/5; 6/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 3/5; 6/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.998 total time=   2.5s
[CV 3/5; 7/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 3/5; 7/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.998 total time=   1.9s
[CV 2/5; 8/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 2/5; 8/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.980 total time=   2.5s
[CV 1/5; 9/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 1/5; 9/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.988 total time=   1.9s
[CV 4/5; 9/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 4/5; 9/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.958 total time=   1.9s
[CV 3/5; 10/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 3/5; 10/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.998 total time=   2.6s
[CV 2/5; 11/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 2/5; 11/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.977 total time=   1.9s
[CV 1/5; 12/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 1/5; 12/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.991 total time=   2.6s
[CV 5/5; 12/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 5/5; 12/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.6s
[CV 1/5; 14/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 1/5; 14/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.989 total time=   2.7s
[CV 5/5; 14/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 5/5; 14/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.986 total time=   2.7s
[CV 5/5; 15/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 5/5; 15/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.986 total time=   1.9s
[CV 4/5; 16/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 4/5; 16/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.952 total time=   2.5s
[CV 3/5; 17/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 4/5; 1/10] START C=1.0, penalty=l2..........................................
[CV 4/5; 1/10] END ...........C=1.0, penalty=l2;, score=0.948 total time=   0.0s
[CV 4/5; 2/10] START C=21.544346900318832, penalty=l2...........................
[CV 4/5; 2/10] END C=21.544346900318832, penalty=l2;, score=0.949 total time=   0.0s
[CV 3/5; 3/10] START C=464.15888336127773, penalty=l2...........................
[CV 3/5; 3/10] END C=464.15888336127773, penalty=l2;, score=0.997 total time=   0.1s
[CV 2/5; 4/10] START C=9999.999999999995, penalty=l2............................
[CV 2/5; 4/10] END C=9999.999999999995, penalty=l2;, score=0.976 total time=   0.3s
[CV 2/5; 5/10] START C=215443.46900318822, penalty=l2...........................
[CV 2/5; 5/10] END C=215443.46900318822, penalty=l2;, score=0.976 total time=   0.1s
[CV 1/5; 6/10] START C=4641588.833612782, penalty=l2............................
[CV 1/5; 6/10] END C=4641588.833612782, penalty=l2;, score=0.999 total time=   0.2s
[CV 2/5; 6/10] START C=4641588.833612782, penalty=l2............................
[CV 2/5; 6/10] END C=4641588.833612782, penalty=l2;, score=0.977 total time=   0.1s
[CV 5/5; 6/10] START C=4641588.833612782, penalty=l2............................
[CV 5/5; 6/10] END C=4641588.833612782, penalty=l2;, score=0.986 total time=   0.5s
[CV 4/5; 7/10] START C=99999999.9999999, penalty=l2.............................
[CV 4/5; 7/10] END C=99999999.9999999, penalty=l2;, score=0.942 total time=   1.2s
[CV 3/5; 9/10] START C=46415888336.12772, penalty=l2............................
[CV 3/5; 9/10] END C=46415888336.12772, penalty=l2;, score=0.981 total time=   0.2s
[CV 5/5; 9/10] START C=46415888336.12772, penalty=l2............................
[CV 5/5; 9/10] END C=46415888336.12772, penalty=l2;, score=0.986 total time=   0.3s
[CV 1/5; 10/10] START C=1000000000000.0, penalty=l2.............................
[CV 1/5; 10/10] END C=1000000000000.0, penalty=l2;, score=0.999 total time=   0.2s
[CV 3/5; 10/10] START C=1000000000000.0, penalty=l2.............................
[CV 3/5; 10/10] END C=1000000000000.0, penalty=l2;, score=0.986 total time=   0.2s
[CV 1/5; 1/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 1/5; 1/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.989 total time=   1.8s
[CV 5/5; 1/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 5/5; 1/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.988 total time=   2.0s
[CV 4/5; 2/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 4/5; 2/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.957 total time=   2.8s
[CV 3/5; 3/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 3/5; 3/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.999 total time=   2.1s
[CV 3/5; 4/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 3/5; 4/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.998 total time=   2.5s
[CV 2/5; 5/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 2/5; 5/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.978 total time=   1.8s
[CV 5/5; 5/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 5/5; 5/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.987 total time=   1.8s
[CV 4/5; 6/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 4/5; 6/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.951 total time=   2.5s
[CV 2/5; 7/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 2/5; 7/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.978 total time=   1.9s
[CV 1/5; 8/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 1/5; 8/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.5s
[CV 5/5; 8/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 5/5; 8/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.987 total time=   2.5s
[CV 1/5; 10/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 1/5; 10/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.988 total time=   2.6s
[CV 5/5; 10/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 5/5; 10/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.988 total time=   2.7s
[CV 5/5; 11/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 5/5; 11/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.989 total time=   1.8s
[CV 4/5; 12/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 4/5; 12/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.954 total time=   2.6s
[CV 3/5; 13/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 3/5; 13/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.999 total time=   1.8s
[CV 2/5; 14/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 2/5; 14/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.979 total time=   2.8s
[CV 1/5; 15/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 1/5; 15/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.989 total time=   2.0s
[CV 4/5; 15/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 4/5; 15/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.950 total time=   1.8s
[CV 3/5; 16/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 3/5; 16/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.998 total time=   2.5s
[CV 2/5; 17/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 2/5; 17/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.975 total time=   1.9s
[CV 1/5; 18/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 1/5; 18/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.990 total time=   2.6s
[CV 5/5; 18/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 5/5; 18/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.989 total time=   2.7s
[CV 5/5; 19/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 2/5; 1/10] START C=1.0, penalty=l2..........................................
[CV 2/5; 1/10] END ...........C=1.0, penalty=l2;, score=0.983 total time=   0.0s
[CV 1/5; 2/10] START C=21.544346900318832, penalty=l2...........................
[CV 1/5; 2/10] END C=21.544346900318832, penalty=l2;, score=0.996 total time=   0.0s
[CV 5/5; 2/10] START C=21.544346900318832, penalty=l2...........................
[CV 5/5; 2/10] END C=21.544346900318832, penalty=l2;, score=0.989 total time=   0.0s
[CV 4/5; 3/10] START C=464.15888336127773, penalty=l2...........................
[CV 4/5; 3/10] END C=464.15888336127773, penalty=l2;, score=0.955 total time=   0.1s
[CV 1/5; 4/10] START C=9999.999999999995, penalty=l2............................
[CV 1/5; 4/10] END C=9999.999999999995, penalty=l2;, score=0.999 total time=   0.2s
[CV 5/5; 4/10] START C=9999.999999999995, penalty=l2............................
[CV 5/5; 4/10] END C=9999.999999999995, penalty=l2;, score=0.981 total time=   0.2s
[CV 4/5; 5/10] START C=215443.46900318822, penalty=l2...........................
[CV 4/5; 5/10] END C=215443.46900318822, penalty=l2;, score=0.956 total time=   0.5s
[CV 2/5; 7/10] START C=99999999.9999999, penalty=l2.............................
[CV 2/5; 7/10] END C=99999999.9999999, penalty=l2;, score=0.977 total time=   0.4s
[CV 5/5; 7/10] START C=99999999.9999999, penalty=l2.............................
[CV 5/5; 7/10] END C=99999999.9999999, penalty=l2;, score=0.984 total time=   0.1s
[CV 2/5; 8/10] START C=2154434690.031878, penalty=l2............................
[CV 2/5; 8/10] END C=2154434690.031878, penalty=l2;, score=0.976 total time=   0.2s
[CV 4/5; 8/10] START C=2154434690.031878, penalty=l2............................
[CV 4/5; 8/10] END C=2154434690.031878, penalty=l2;, score=0.953 total time=   1.9s
[CV 3/5; 1/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 3/5; 1/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.999 total time=   1.9s
[CV 2/5; 2/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 2/5; 2/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.976 total time=   2.8s
[CV 5/5; 2/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 5/5; 2/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.987 total time=   3.3s
[CV 1/5; 4/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 1/5; 4/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.5s
[CV 5/5; 4/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 5/5; 4/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.6s
[CV 1/5; 6/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 1/5; 6/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.989 total time=   2.5s
[CV 5/5; 6/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 5/5; 6/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.988 total time=   2.6s
[CV 5/5; 7/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 5/5; 7/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.986 total time=   1.8s
[CV 4/5; 8/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 4/5; 8/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.950 total time=   2.5s
[CV 3/5; 9/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 3/5; 9/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.999 total time=   1.9s
[CV 2/5; 10/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 2/5; 10/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.978 total time=   2.6s
[CV 1/5; 11/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 1/5; 11/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.990 total time=   1.9s
[CV 4/5; 11/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 4/5; 11/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.957 total time=   1.8s
[CV 3/5; 12/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 3/5; 12/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.999 total time=   2.6s
[CV 2/5; 13/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 2/5; 13/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.978 total time=   1.8s
[CV 5/5; 13/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 5/5; 13/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.986 total time=   1.8s
[CV 4/5; 14/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 4/5; 14/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.950 total time=   2.8s
[CV 3/5; 15/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 3/5; 15/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.998 total time=   1.8s
[CV 2/5; 16/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 2/5; 16/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.979 total time=   2.5s
[CV 5/5; 16/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 5/5; 16/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.986 total time=   2.5s
[CV 5/5; 17/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 5/5; 17/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.988 total time=   1.9s
[CV 4/5; 18/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 4/5; 18/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.957 total time=   2.7s
[CV 3/5; 19/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 3/5; 19/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.999 total time=   1.9s
[CV 2/5; 20/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 2/5; 20/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.979 total time=   2.6s
[CV 1/5; 21/24] START max_depth=15, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 3/5; 1/10] START C=1.0, penalty=l2..........................................
[CV 3/5; 1/10] END ...........C=1.0, penalty=l2;, score=1.000 total time=   0.0s
[CV 3/5; 2/10] START C=21.544346900318832, penalty=l2...........................
[CV 3/5; 2/10] END C=21.544346900318832, penalty=l2;, score=0.999 total time=   0.0s
[CV 2/5; 3/10] START C=464.15888336127773, penalty=l2...........................
[CV 2/5; 3/10] END C=464.15888336127773, penalty=l2;, score=0.977 total time=   0.1s
[CV 3/5; 4/10] START C=9999.999999999995, penalty=l2............................
[CV 3/5; 4/10] END C=9999.999999999995, penalty=l2;, score=0.998 total time=   0.2s
[CV 1/5; 5/10] START C=215443.46900318822, penalty=l2...........................
[CV 1/5; 5/10] END C=215443.46900318822, penalty=l2;, score=0.999 total time=   0.2s
[CV 5/5; 5/10] START C=215443.46900318822, penalty=l2...........................
[CV 5/5; 5/10] END C=215443.46900318822, penalty=l2;, score=0.983 total time=   0.2s
[CV 4/5; 6/10] START C=4641588.833612782, penalty=l2............................
[CV 4/5; 6/10] END C=4641588.833612782, penalty=l2;, score=0.955 total time=   2.2s
[CV 2/5; 10/10] START C=1000000000000.0, penalty=l2.............................
[CV 2/5; 10/10] END C=1000000000000.0, penalty=l2;, score=0.977 total time=   0.2s
[CV 5/5; 10/10] START C=1000000000000.0, penalty=l2.............................
[CV 5/5; 10/10] END C=1000000000000.0, penalty=l2;, score=0.986 total time=   0.2s
[CV 2/5; 1/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 2/5; 1/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.979 total time=   1.9s
[CV 3/5; 2/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 3/5; 2/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.999 total time=   2.8s
[CV 2/5; 3/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 2/5; 3/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.978 total time=   2.1s
[CV 5/5; 3/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 5/5; 3/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.987 total time=   2.1s
[CV 4/5; 4/24] START max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 4/5; 4/24] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.956 total time=   2.6s
[CV 3/5; 5/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 3/5; 5/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.998 total time=   1.8s
[CV 2/5; 6/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 2/5; 6/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.979 total time=   2.6s
[CV 1/5; 7/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 1/5; 7/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.989 total time=   1.8s
[CV 4/5; 7/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 4/5; 7/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.951 total time=   1.9s
[CV 3/5; 8/24] START max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 3/5; 8/24] END max_depth=5, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.999 total time=   2.5s
[CV 2/5; 9/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 2/5; 9/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.978 total time=   1.9s
[CV 5/5; 9/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 5/5; 9/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.988 total time=   1.9s
[CV 4/5; 10/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 4/5; 10/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.957 total time=   2.6s
[CV 3/5; 11/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 3/5; 11/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.999 total time=   1.9s
[CV 2/5; 12/24] START max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 2/5; 12/24] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.977 total time=   2.6s
[CV 1/5; 13/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 1/5; 13/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.989 total time=   1.8s
[CV 4/5; 13/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500
[CV 4/5; 13/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=500;, score=0.951 total time=   1.8s
[CV 3/5; 14/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700
[CV 3/5; 14/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=2, n_estimators=700;, score=0.998 total time=   2.9s
[CV 2/5; 15/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500
[CV 2/5; 15/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=500;, score=0.978 total time=   1.9s
[CV 1/5; 16/24] START max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700
[CV 1/5; 16/24] END max_depth=10, max_features=sqrt, min_samples_leaf=20, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.6s
[CV 1/5; 17/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 1/5; 17/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.990 total time=   1.9s
[CV 4/5; 17/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500
[CV 4/5; 17/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=500;, score=0.955 total time=   1.9s
[CV 3/5; 18/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700
[CV 3/5; 18/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=700;, score=0.999 total time=   2.7s
[CV 2/5; 19/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500
[CV 2/5; 19/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=500;, score=0.978 total time=   2.0s
[CV 1/5; 20/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 1/5; 20/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.990 total time=   2.7s
[CV 5/5; 20/24] START max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700
[CV 5/5; 20/24] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=700;, score=0.988 total time=   2.6s

GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,
             param_grid={'max_depth': [5, 10, 15], 'max_features': ['sqrt'],
                         'min_samples_leaf': [10, 20],
                         'min_samples_split': [2, 5],
                         'n_estimators': [500, 700]},
             scoring='roc_auc', verbose=10)

In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
3.4. Dummy Classifier

# Dummy Classifier
dummy = DummyClassifier()
dummy.fit(X_train_under, y_train_under)

DummyClassifier()

In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
4. Model Evaluation
4.1. Find ROC scores for all models

image.pngimage.png

def plot_roc_curves(X, y, models, model_names, figsize=(20,18)):
    """
    Plots ROC curves for a list of models.

    Parameters:
    X (numpy.ndarray or pandas.DataFrame): input features for the models
    y (numpy.ndarray or pandas.DataFrame): target variable
    models (list): list of models to compare
    model_names (list): list of model names to display on the plot
    figsize (tuple): size of the figure to display the plot

    Returns:
    None
    """
    fig, ax = plt.subplots(figsize=figsize)

    # Loop over models and plot ROC curve
    for i, model in enumerate(models):
        y_pred = list(model.predict_proba(X)[:, 1])
        fpr, tpr, threshold = metrics.roc_curve(y, y_pred)
        roc_auc = metrics.auc(fpr, tpr)
        plt.plot(fpr, tpr, label=(model_names[i] + ' AUC = %0.4f' % roc_auc), linewidth=2.0)

    ax.grid(False)
    ax.tick_params(length=6, width=2, labelsize=30, grid_color='r', grid_alpha=0.5)
    leg = plt.legend(loc='lower right', prop={'size': 25})
    leg.get_frame().set_edgecolor('b')
    plt.title('Receiver Operating Characteristic (ROC)', fontsize=40)
    plt.plot([0, 1], [0, 1], 'r--')
    plt.xlim([-.02, 1.02])
    plt.ylim([-.02, 1.02])
    plt.ylabel('True Positive Rate', fontsize=30)
    plt.xlabel('False Positive Rate', fontsize=30)
#     plt.show()

# Define the list of models to compare
models = [logistic_grid.best_estimator_, gnb_best, rf_grid.best_estimator_, dummy]
model_names = ['Logit', 'Naive Bayes', 'Random Forest', 'Dummy']

# Plot ROC curves for in-sample data
plot_roc_curves(X_val_under, y_val_under, models, model_names)

# Save the plot as PNG file
plt.savefig('roc_insample.png');

# Plot ROC curves for out-of-sample data
plot_roc_curves(X_test_std, y_test, models, model_names)

# Save the plot as PNG file
plt.savefig('roc_outsample.png');

    Recall (True Positive Rate): This metric measures the percentage of all fraudulent transactions that the model correctly identifies as fraudulent.
    Precision: This metric indicates the percentage of items that the model labels as fraud that are actually fraudulent.
    False Positive Rate: This metric measures the percentage of non-fraudulent transactions that the model incorrectly labels as fraudulent.
    Accuracy: This metric reflects how often the model is correct in its predictions overall. However, it can be misleading in the case of imbalanced data or fraud detection.
    F1 score: This metric is a combination of precision and recall, taking both false positives and false negatives into account. It's a weighted average of precision and recall and is usually more useful than accuracy, especially when dealing with uneven classes.

4.2. Determine the optimal threshold for each model.

✅ The function find_best_threshold() can be used to determine the optimal threshold for a given model. The optimal threshold is the value that maximizes the F1 score, a measure that combines precision and recall, for a binary classification problem.

✅ The function takes two arguments: model is the trained model, and num_steps is the number of steps in the threshold range to iterate over.

✅ The function first initializes variables for the highest F1 score, the best threshold, and the best accuracy, recall, and precision scores. It then iterates over a range of thresholds from 0 to 1, with num_steps steps. For each threshold, it predicts the target variable using the given threshold and calculates the F1 score, accuracy, recall, and precision scores. If the F1 score is higher than the current highest F1 score, it updates the best threshold and evaluation metrics.

✅ After iterating over all the thresholds, the function returns the best threshold and the corresponding F1 score, accuracy, recall, and precision scores.

✅ The math equation to find the F1 score is:

F1 = 2 * (precision * recall) / (precision + recall)

where

    precision = TP / (TP + FP)
    recall = TP / (TP + FN)
    TP: True Positive (model predicts positive and it is positive)
    FP: False Positive (model predicts positive but it is negative)
    FN: False Negative (model predicts negative but it is positive)

# Define a function to find the best threshold for a given model
def find_best_threshold(model, num_steps):
    highest_f1 = 0
    best_threshold = 0
    best_acc = 0
    best_rec = 0
    best_pre = 0
    # Iterate over a range of thresholds
    for threshold in np.linspace(0, 1, num_steps):
        # Predict the target variable using the given threshold
        y_predict = (model.predict_proba(X_val_under)[:, 1] >= threshold)
        # Calculate various evaluation metrics
        f1 = f1_score(y_val_under, y_predict)
        acc = accuracy_score(y_val_under, y_predict)
        rec = recall_score(y_val_under, y_predict)
        pre = precision_score(y_val_under, y_predict)
        # Update the best threshold and metrics if F1 score improves
        if f1 > highest_f1:
            best_threshold, highest_f1, best_acc, best_rec, best_pre = \
                threshold, f1, acc, rec, pre
    # Return the best threshold and evaluation metrics
    return best_threshold, highest_f1, best_acc, best_rec, best_pre

# Define a list of models and their names
models = [logistic_grid, gnb_best, rf_grid]
model_names = ["Logistic Regression", "Naive-Bayes", "Random Forest"]

# Create an empty list to store the results
chart = list()

# Iterate over the models and find the best threshold for each one
for item, name in zip(models, model_names):
    best_thresh, high_f1, high_acc, high_rec, high_pre = find_best_threshold(item, 20)
    # Append the results to the chart list
    chart.append([name, best_thresh, high_f1, high_acc, high_rec, high_pre])

# Create a pandas dataframe from the chart list and display it
chart = pd.DataFrame(chart, columns=['Model', 'Best Threshold', 'F1 Score', 'Accuracy', 'Recall', 'Precision'])
chart.to_csv('model_evaluation_scores.csv')
chart

	Model 	Best Threshold 	F1 Score 	Accuracy 	Recall 	Precision
0 	Logistic Regression 	0.842105 	0.916667 	0.919192 	0.888889 	0.946237
1 	Naive-Bayes 	0.052632 	0.870466 	0.873737 	0.848485 	0.893617
2 	Random Forest 	0.421053 	0.925532 	0.929293 	0.878788 	0.977528
4.3. Confusion Matrix

def make_confusion_matrix_val(model, threshold=0.5):
    """
    Create a confusion matrix plot for the given model and threshold. 
    
    Parameters:
    -----------
    model : sklearn classifier
        The classification model to evaluate.
    threshold : float, default=0.5
        Probability threshold for binary classification.
        
    Returns:
    --------
    None
    
    """
    # Predict class 1 if probability of being in class 1 is greater than threshold
    # (model.predict(X_test) does this automatically with a threshold of 0.5)
    y_predict = (model.predict_proba(X_val_under)[:, 1] >= threshold)
    
    # calculate the confusion matrix
    fraud_confusion = confusion_matrix(y_val_under, y_predict)
    
    # plot the confusion matrix as heatmap
    plt.figure(dpi=100)
    sns.set(font_scale=1)
    sns.heatmap(fraud_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',
           xticklabels=['Not Fraud', 'Fraud'],
           yticklabels=['Not Fraud', 'Fraud']);
    
    # calculate TP, FP, FN, and TN values from the confusion matrix
    TP = fraud_confusion[0][0]
    FP = fraud_confusion[0][1]
    FN = fraud_confusion[1][0]
    TN = fraud_confusion[1][1]
    
    # rotate y-axis ticks
    plt.yticks(rotation = 0)
    
    # set plot title, x and y labels
    plt.title('Predicted vs. Actual',fontname = '.SF Compact Display',fontsize = 20,pad = 10);
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

# Create a confusion matrix for the Random Forest model with a threshold of 0.421 on the validation data
make_confusion_matrix_val(rf_grid, threshold=0.421)

# Save the plot as PNG file
plt.savefig('confusion_matrix_val_random_forest.png');

# Create a confusion matrix for the Logistic Regression model with a threshold of 0.842 on the validation data
make_confusion_matrix_val(logistic_grid, threshold=0.842)

# Save the plot as PNG file
plt.savefig('confusion_matrix_val_logistic_regression.png');

def make_confusion_matrix_test(model, threshold=0.5):
    """
    Generates a confusion matrix for a given model on the test dataset, given a threshold.

    Args:
    - model: a trained machine learning model
    - threshold: threshold for binary classification

    Returns: None
    """

    # Predict class 1 if probability of being in class 1 is greater than threshold
    y_predict = (model.predict_proba(X_test_std)[:, 1] >= threshold)

    # Generate confusion matrix
    fraud_confusion = confusion_matrix(y_test, y_predict)

    # Plot heatmap of confusion matrix
    plt.figure(dpi=100)
    sns.set(font_scale=1)
    sns.heatmap(fraud_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',
                xticklabels=['Not Fraud', 'Fraud'],
                yticklabels=['Not Fraud', 'Fraud'])

    # Calculate TP, FP, FN, TN
    TP = fraud_confusion[0][0]
    FP = fraud_confusion[0][1]
    FN = fraud_confusion[1][0]
    TN = fraud_confusion[1][1]

    # Add title, labels and rotate y-tick labels
    plt.yticks(rotation=0)
    plt.title('Predicted vs. Actual', fontname='.SF Compact Display', fontsize=20, pad=10)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

# Generate confusion matrix for random forest model on test dataset
make_confusion_matrix_test(rf_grid, threshold=0.421)

# Save the plot as PNG file
plt.savefig('confusion_matrix_test_random_forest.png');

# Generate confusion matrix for logistic regression model on test dataset
make_confusion_matrix_test(logistic_grid, threshold=0.842)

# Save the plot as PNG file
plt.savefig('confusion_matrix_test_logistic_regression.png');

References

    Kaggle Dataset: Credit Card Fraud Detection
    Github Repo - HERE
    Kaggle Project - HERE
    Detail Explanation about the code on MEDIUM

